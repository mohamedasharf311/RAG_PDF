import os
import tempfile
import numpy as np
import faiss
import nltk
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
import gradio as gr
import time

# ==================== ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… ====================
class FlowRAGSystem:
    def __init__(self):
        self.model = None
        self.index = None
        self.chunks = None
        self.current_file = None
        self.is_ready = False
    
    def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…"""
        try:
            # ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK
            try:
                nltk.download('punkt', quiet=True)
                nltk.download('punkt_tab', quiet=True)
            except:
                pass
            
            self.model = SentenceTransformer(
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
            self.is_ready = True
            return True
        except Exception as e:
            return f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}"
    
    def process_pdf(self, pdf_file):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù PDF"""
        try:
            self.current_file = pdf_file.name
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(pdf_file.read())
                pdf_path = tmp_file.name
            
            # Ù‚Ø±Ø§Ø¡Ø© PDF
            reader = PdfReader(pdf_path)
            pages_data = []
            
            for i, page in enumerate(reader.pages):
                text = page.extract_text()
                if text and text.strip():
                    pages_data.append({
                        'page': i + 1,
                        'text': text.strip()
                    })
            
            if not pages_data:
                os.unlink(pdf_path)
                return "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Øµ ÙÙŠ Ø§Ù„Ù…Ù„Ù"
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ
            self.chunks = []
            for page in pages_data:
                words = page['text'].split()
                
                # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ 200 ÙƒÙ„Ù…Ø© Ù…Ø¹ ØªØ¯Ø§Ø®Ù„ 40
                chunk_size = 200
                overlap = 40
                
                start = 0
                while start < len(words):
                    end = start + chunk_size
                    chunk_words = words[start:end]
                    
                    if chunk_words:
                        self.chunks.append({
                            'text': ' '.join(chunk_words),
                            'page': page['page'],
                            'word_count': len(chunk_words)
                        })
                    
                    start += chunk_size - overlap
            
            # Ø¥Ù†Ø´Ø§Ø¡ embeddings
            if len(self.chunks) > 0:
                chunk_texts = [chunk['text'] for chunk in self.chunks]
                embeddings = self.model.encode(
                    chunk_texts,
                    normalize_embeddings=True,
                    show_progress_bar=False
                )
                
                # Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³
                dimension = embeddings.shape[1]
                self.index = faiss.IndexFlatIP(dimension)
                faiss.normalize_L2(embeddings)
                self.index.add(embeddings)
            else:
                os.unlink(pdf_path)
                return "âŒ Ù„Ù… ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø£ÙŠ Ø£Ø¬Ø²Ø§Ø¡ Ù†ØµÙŠØ©"
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
            os.unlink(pdf_path)
            
            return f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ù†Ø¬Ø§Ø­!\nğŸ“Š {len(pages_data)} ØµÙØ­Ø© â†’ {len(self.chunks)} Ø¬Ø²Ø¡ Ù†ØµÙŠ"
            
        except Exception as e:
            return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© PDF: {str(e)}"
    
    def search(self, query, top_k=3):
        """Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯"""
        if not self.is_ready or self.index is None:
            return "âŒ ÙŠØ±Ø¬Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³ØªÙ†Ø¯ Ø£ÙˆÙ„Ø§Ù‹"
        
        try:
            query_embedding = self.model.encode([query], normalize_embeddings=True)
            scores, indices = self.index.search(query_embedding, top_k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if 0 <= idx < len(self.chunks):
                    chunk = self.chunks[idx]
                    
                    # ØªØ­Ø¯ÙŠØ¯ Ù„ÙˆÙ† Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                    similarity_score = float(score)
                    if similarity_score >= 0.5:
                        sim_color = "#28a745"  # Ø£Ø®Ø¶Ø±
                        sim_text = "Ù…Ù…ØªØ§Ø²"
                    elif similarity_score >= 0.3:
                        sim_color = "#ffc107"  # Ø£ØµÙØ±
                        sim_text = "Ø¬ÙŠØ¯"
                    else:
                        sim_color = "#dc3545"  # Ø£Ø­Ù…Ø±
                        sim_text = "Ø¶Ø¹ÙŠÙ"
                    
                    results.append(f"""
                    <div style="background: #f8f9fa; border-radius: 10px; padding: 1.5rem; 
                    margin: 1rem 0; border-left: 5px solid {sim_color}; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                        <h4 style="margin-top: 0;">ğŸ† Ø§Ù„Ù†ØªÙŠØ¬Ø© #{i+1}</h4>
                        <p style="margin-bottom: 0.5rem;">
                            <span style="color: {sim_color}; font-weight: bold;">Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {score*100:.1f}% ({sim_text})</span> | 
                            ğŸ“– Ø§Ù„ØµÙØ­Ø©: {chunk['page']} | 
                            ğŸ”¢ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {chunk['word_count']}
                        </p>
                        <hr style="margin: 0.5rem 0;">
                        <p>{chunk['text'][:400]}...</p>
                    </div>
                    """)
            
            if not results:
                return "âŒ Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯"
            
            return f"<h3>ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ù†ØªÙŠØ¬Ø©:</h3>" + "".join(results)
            
        except Exception as e:
            return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {str(e)}"

# ==================== Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… ====================
rag_system = FlowRAGSystem()
init_result = rag_system.initialize()

# ==================== ÙˆØ§Ø¬Ù‡Ø© Gradio ====================
with gr.Blocks(title="ğŸ¤– Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª", theme=gr.themes.Soft()) as demo:
    
    # Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    gr.Markdown("""
    # ğŸ¤– Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
    ### Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ Ù…ØªÙ‚Ø¯Ù… ÙÙŠ Ù…Ù„ÙØ§Øª PDF - ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
    """)
    
    # Ù…Ù†Ø·Ù‚Ø© Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
    if init_result is not True:
        gr.Warning(f"âš ï¸ {init_result}")
    else:
        gr.Info("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…")
    
    with gr.Row():
        with gr.Column(scale=2):
            # Ù‚Ø³Ù… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù
            with gr.Group():
                gr.Markdown("## ğŸ“ Ø±ÙØ¹ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯")
                file_input = gr.File(
                    label="Ø§Ø®ØªØ± Ù…Ù„Ù PDF",
                    file_types=[".pdf"],
                    type="binary"
                )
                process_btn = gr.Button("ğŸš€ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯", variant="primary")
                process_output = gr.Markdown(label="Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
            
            # Ù‚Ø³Ù… Ø§Ù„Ø¨Ø­Ø«
            with gr.Group():
                gr.Markdown("## ğŸ’¬ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯")
                question_input = gr.Textbox(
                    label="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§",
                    placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¯ÙÙ‚ØŸ Ø£Ùˆ What is flow state?",
                    lines=3
                )
                
                with gr.Row():
                    top_k_slider = gr.Slider(
                        minimum=1, maximum=5, value=3,
                        label="Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"
                    )
                    search_btn = gr.Button("ğŸ” Ø§Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯", variant="primary")
                
                search_output = gr.HTML(label="Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«")
        
        with gr.Column(scale=1):
            # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
            with gr.Group():
                gr.Markdown("## ğŸ’¡ Ø£Ø³Ø¦Ù„Ø© Ø³Ø±ÙŠØ¹Ø©")
                
                example_questions = [
                    "Ù…Ø§ Ù‡ÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¯ÙÙ‚ØŸ",
                    "What is flow state?",
                    "Ù…Ø§ Ù‡ÙŠ Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø«Ù„Ù‰ØŸ",
                    "ÙƒÙŠÙ ÙŠØ­Ù‚Ù‚ Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ø§Ù„Ø³Ø¹Ø§Ø¯Ø© ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ØŸ",
                    "Ù…Ø§ Ù‡Ùˆ Ø¯ÙˆØ± Ø§Ù„ØªØ±ÙƒÙŠØ² ÙÙŠ Ø§Ù„ØªØ¯ÙÙ‚ØŸ"
                ]
                
                for question in example_questions:
                    gr.Button(
                        question,
                        size="sm",
                    ).click(
                        fn=lambda q=question: q,
                        inputs=[],
                        outputs=[question_input]
                    )
            
            with gr.Group():
                gr.Markdown("## ğŸ¯ Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ø¨Ø­Ø«")
                gr.Markdown("""
                **Ù„Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:**
                
                â€¢ Ø§Ø³ØªØ®Ø¯Ù… Ù…ØµØ·Ù„Ø­Ø§Øª Ù…Ø­Ø¯Ø¯Ø©  
                â€¢ Ø¬Ø±Ø¨ Ø§Ù„Ù„ØºØªÙŠÙ† (Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)  
                â€¢ Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ø¶Ø­Ø©  
                
                **Ù…Ø«Ø§Ù„:**  
                âœ… "Ù…Ø§ Ù‡ÙŠ Ø®ØµØ§Ø¦Øµ flow stateØŸ"  
                âŒ "Ø§Ø´Ø±Ø­ Ù„ÙŠ"
                """)
            
            with gr.Group():
                gr.Markdown("## ğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
                status_text = gr.Markdown("ğŸ“„ Ù„Ù… ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙŠ Ù…Ø³ØªÙ†Ø¯ Ø¨Ø¹Ø¯")
                
                # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
                def update_status():
                    if rag_system.current_file:
                        file_info = f"ğŸ“„ Ø§Ù„Ù…Ù„Ù: {rag_system.current_file}"
                        if rag_system.chunks:
                            chunks_info = f" | ğŸ“Š Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡: {len(rag_system.chunks)}"
                            if rag_system.index:
                                vectors_info = f" | ğŸ§® Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª: {rag_system.index.ntotal}"
                                return file_info + chunks_info + vectors_info
                            return file_info + chunks_info
                        return file_info
                    return "ğŸ“„ Ù„Ù… ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙŠ Ù…Ø³ØªÙ†Ø¯ Ø¨Ø¹Ø¯"
                
                status_display = gr.Markdown(update_status())
    
    # Ù†ØµØ§Ø¦Ø­ Ø¥Ø¶Ø§ÙÙŠØ©
    gr.Markdown("---")
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ“š Ø¹Ù† Ø§Ù„Ù†Ø¸Ø§Ù…")
            gr.Markdown("""
            **Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:**
            
            â€¢ ğŸ¤– **Sentence Transformers** - Ù†Ù…Ø§Ø°Ø¬ embedding Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù„ØºØ§Øª  
            â€¢ âš¡ **FAISS** - Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹ ÙÙŠ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª  
            â€¢ ğŸ“„ **PyPDF** - Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF  
            â€¢ ğŸŒ **Gradio** - ÙˆØ§Ø¬Ù‡Ø© Ù…Ø³ØªØ®Ø¯Ù… ØªÙØ§Ø¹Ù„ÙŠØ©
            """)
        
        with gr.Column():
            gr.Markdown("### ğŸŒ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù„ØºÙˆÙŠ")
            gr.Markdown("""
            **Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:**
            
            â€¢ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬  
            â€¢ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© - Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬  
            â€¢ Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©ØŒ Ø§Ù„Ø¥Ø³Ø¨Ø§Ù†ÙŠØ©ØŒ Ø§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ© - Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            
            **Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:**  
            âœ“ Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ Ø°ÙƒÙŠ  
            âœ“ Ù†ØªØ§Ø¦Ø¬ Ù…Ø±ØªØ¨Ø© Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø©  
            âœ“ Ø¯Ø¹Ù… Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø©
            """)
    
    # ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
    gr.Markdown("---")
    gr.Markdown("""
    <div style="text-align: center; color: #666;">
        <p>ğŸ¤– Ù†Ø¸Ø§Ù… RAG Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª | Ø¥ØµØ¯Ø§Ø± HuggingFace Spaces</p>
        <p>ØªÙ‚Ù†ÙŠØ©: FAISS + Sentence Transformers + Gradio | ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©</p>
    </div>
    """)
    
    # ==================== Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø­Ø¯Ø§Ø« ====================
    def process_file(file):
        if file is None:
            return "âš ï¸ ÙŠØ±Ø¬Ù‰ Ø§Ø®ØªÙŠØ§Ø± Ù…Ù„Ù PDF Ø£ÙˆÙ„Ø§Ù‹"
        
        result = rag_system.process_pdf(file)
        return result
    
    def search_query(question, top_k):
        if not question:
            return "âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„"
        
        return rag_system.search(question, int(top_k))
    
    # Ø±Ø¨Ø· Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
    process_btn.click(
        fn=process_file,
        inputs=[file_input],
        outputs=[process_output]
    ).then(
        fn=update_status,
        inputs=[],
        outputs=[status_display]
    )
    
    search_btn.click(
        fn=search_query,
        inputs=[question_input, top_k_slider],
        outputs=[search_output]
    )
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¶ØºØ· Enter ÙÙŠ Ø­Ù‚Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„
    question_input.submit(
        fn=search_query,
        inputs=[question_input, top_k_slider],
        outputs=[search_output]
    )

# ==================== ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ====================
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
